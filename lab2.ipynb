{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SebastianOrmaechea/Reinforcement-Learning/blob/main/Lab2RL_G_O_V.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 2: Introducción al aprendizaje por refuerzos\n",
        "\n",
        "Curso Aprendizaje por Refuerzos, Diplomatura en Ciencia de Datos, Aprendizaje Automático y sus Aplicaciones\n",
        "\n",
        "FaMAF, 2022\n",
        "\n",
        "\n",
        "\n",
        "<font color='red'>Integrantes:\n",
        "\n",
        "\n",
        "<font color='red'> * Garay, Carolina del Valle\n",
        "\n",
        "<font color='red'> * Ormaechea, Sebastián Gabriel\n",
        "\n",
        "<font color='red'> * Venchiarutti, Gustavo\n"
      ],
      "metadata": {
        "id": "Zyn84lT_aYjT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SytTR8K1oII-"
      },
      "source": [
        "# Introducción"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BFrF74foIJD"
      },
      "source": [
        "Créditos:\n",
        "\n",
        "* Documentación y repo de Stable-baselines https://stable-baselines3.readthedocs.io.\n",
        "    * Tutorial sobre SB3: https://github.com/araffin/rl-tutorial-jnrr19.\n",
        "* Documentación y repo de OpenAI Gym https://github.com/openai/gym/blob/master/docs/.\n",
        "    * Crear un entorno https://github.com/openai/gym/blob/master/docs/creating-environments.md. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlZ0ehGZoIJD"
      },
      "source": [
        "Stable-baselines3: framework de deep RL que provee interfaces para ejecutar y adaptar algoritmos de RL \"al estilo scikit-learn\". Permite utilizar agentes abstrayéndonos de los detalles de bajo nivel de abstracción referentes a la implementación del algoritmo$^1$\n",
        "\n",
        "Además, ofrece herramientas muy útiles como\n",
        "\n",
        "* Monitores que permiten ver el rendimiento del agente según se desempeña en el entorno, sin tener que esperar a que finalice de entrenar.\n",
        "* Callbacks que permiten accionar eventos cuando se cumplen algunas condiciones en el entrenamiento de nuestro agente (por ejemplo, detenerlo si la recompensa recibida es menor a cierto umbral tras un cierto período de tiempo).\n",
        "\n",
        "\n",
        "Documentación https://stable-baselines3.readthedocs.io\n",
        "\n",
        "Es un fork activamente mantenido de [OpenAI baselines](https://github.com/openai/baselines)\n",
        "\n",
        "La versión 3 cambia el framework subyacente de Tensorflow a Pytorch y está activamente en desarrollo; no obstante la versión 2 es completamente funcional\n",
        "\n",
        "$^1$ no obstante, al igual que sucede generalmente con librerías de ML: \n",
        "\n",
        "* Siempre es bueno tener en mente las características, ventajas y desventajas del algoritmo utilizado, pues de eso depende mucho la convergencia de nuestra solución, especialmente cuando se emplean entornos adaptados para nuestras necesidades. \n",
        "\n",
        "* Esta librería, al igual que demás frameworks generales de RL, están muy probadas en entornos estándares de RL como Atari o PyBullet. No obstante, es posible que nuestro entorno o nuestras necesidades difieran significativamente, lo que hace que en algunos casos haya que meter mano directo en el código de los algoritmos/librería."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4SWX4wuoIJE"
      },
      "source": [
        "# Interfaz básica stable-baselines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kYO6iWl-_vo"
      },
      "outputs": [],
      "source": [
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SD-FSSuKoIJF"
      },
      "source": [
        "### Instalación de Stable-baselines\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttqOLol_oIJG"
      },
      "outputs": [],
      "source": [
        "#@title Instalación (no modificar)\n",
        "!pip install stable-baselines3[extra] "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vO4mgIsO6hEt"
      },
      "source": [
        "Desde Windows, además, instalar: \n",
        "* Microsoft Visual C++ desde https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
        "* PyType, mediante `conda install -c conda-forge pytype`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUUkScRxoIJQ"
      },
      "source": [
        "### Instalación de RLBaselinesZoo (Opcional!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiKPJUfdoIJQ"
      },
      "source": [
        "Desde Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZM_pM0mIoIJQ"
      },
      "outputs": [],
      "source": [
        "#@title Instalación de RLBaselinesZoo (no modificar)\n",
        "\n",
        "# Estamos en Colab?\n",
        "\n",
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    !git clone --recursive --depth 1 https://github.com/DLR-RM/rl-baselines3-zoo\n",
        "    !cd rl-baselines3-zoo/\n",
        "    !apt-get install swig cmake ffmpeg\n",
        "    !pip install -r /content/rl-baselines3-zoo/requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzBEoyPzoIJR"
      },
      "source": [
        "Desde Linux, ejecutando\n",
        "\n",
        "    git clone --recursive --depth 1 https://github.com/DLR-RM/rl-baselines3-zoo\n",
        "    cd rl-baselines3-zoo/\n",
        "    conda install swig\n",
        "    pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPUpOonyoIJH"
      },
      "source": [
        "## Ejecución de un algoritmo de RL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Morjd1dFoIJH"
      },
      "source": [
        "### Importaciones/inicializaciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OG7i44kqoIJH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from subprocess import Popen, PIPE\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import gym\n",
        "from gym import spaces\n",
        "#from gym.envs.registration import register\n",
        "\n",
        "from stable_baselines3 import DQN, PPO\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "\n",
        "os.makedirs('logs', exist_ok=True)\n",
        "\n",
        "cwd = os.getcwd()\n",
        "\n",
        "%matplotlib inline\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f5JFC7AoIJK"
      },
      "source": [
        "### Renderización"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5nIL0TuwoIJK"
      },
      "outputs": [],
      "source": [
        "if not IN_COLAB:\n",
        "\n",
        "    obs = env.reset()\n",
        "    for i in range(1000):\n",
        "        action, _states = model.predict(obs, deterministic=True)\n",
        "        obs, reward, done, info = env.step(action)\n",
        "        env.render()\n",
        "        if done:\n",
        "          obs = env.reset()\n",
        "\n",
        "    env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFYyMrvboIJK"
      },
      "source": [
        "#### Ver rendimiento del agente en tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pC1hqla2oIJK",
        "outputId": "bc8c5812-f1d4-4adf-9857-ce167a7370d3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<stable_baselines3.dqn.dqn.DQN at 0x7f78f51ca0a0>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "venv = make_vec_env(lambda: gym.make('CartPole-v1'), n_envs=1)\n",
        "\n",
        "model = DQN('MlpPolicy', venv, tensorboard_log='tensorboard/')\n",
        "model.learn(total_timesteps=100000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rldIL1u4oIJL"
      },
      "source": [
        "Para verlo en tensorboard, correr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4Db-mCxoIJL"
      },
      "source": [
        "`tensorboard --logdir=tensorboard/`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2u0_1dk5oIJL"
      },
      "source": [
        "### Monitor\n",
        "\n",
        "Vamos a crear un monitor para loguear nuestro agente en la carpeta logs. Nuestro monitor guardará datos de recompensa (r), duración (l) y tiempo total (t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7uF7sQdoIJL",
        "outputId": "cc93585c-50ef-4824-e969-71dc4b43b00b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<stable_baselines3.dqn.dqn.DQN at 0x7f780b0bfdf0>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "env = gym.make('CartPole-v1')\n",
        "env = Monitor(env, 'logs/')  # reemplazamos env por su monitor\n",
        "\n",
        "model = DQN('MlpPolicy', env, )\n",
        "model.learn(total_timesteps=10000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yVVSGjvoIJM"
      },
      "source": [
        "### Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47XdjvaSoIJM",
        "outputId": "fe9535b0-3f5b-48e9-8e92-2362508abc64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=1000, episode_reward=9.60 +/- 0.80\n",
            "Episode length: 9.60 +/- 0.80\n",
            "New best mean reward!\n",
            "Eval num_timesteps=2000, episode_reward=9.20 +/- 0.75\n",
            "Episode length: 9.20 +/- 0.75\n",
            "Eval num_timesteps=3000, episode_reward=9.80 +/- 0.75\n",
            "Episode length: 9.80 +/- 0.75\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<stable_baselines3.dqn.dqn.DQN at 0x7f780a8098e0>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "env = gym.make('CartPole-v1')\n",
        "\n",
        "callbacks = []  # lista de callbacks a usar, pueden ser varios\n",
        "\n",
        "# callback para detener entrenamiento al alcanzar recompensa de 9.8\n",
        "# (es una recompensa muy baja, pero la establecemos a fines demostrativos)\n",
        "#que deje de entrenar a un cierto umbral de recompensa\n",
        "stop_training_callback = StopTrainingOnRewardThreshold(reward_threshold=9.8)\n",
        "\n",
        "# al crear EvalCallback, se asocia el mismo con stop_training_callback\n",
        "callbacks.append(EvalCallback(Monitor(env, 'logs/'), \n",
        "                              eval_freq=1000,\n",
        "                              callback_on_new_best=stop_training_callback))\n",
        "\n",
        "# la semilla aleatoria hace que las ejecuciones sean determinísticas\n",
        "model = DQN('MlpPolicy', env, seed=42)\n",
        "model.learn(total_timesteps=10000, callback=callbacks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCPD7oaNoIJM"
      },
      "source": [
        "### Ejecutar agente RL en múltiples ambientes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNiTT2AzoIJM"
      },
      "source": [
        "Esta librería provee una interfaz para ejecutar agentes en varias instancias de un mismo entorno a la vez (*vectorized environments*), de modo tal que se habilite la ejecución paralela y de otras funcionalidades útiles.\n",
        "\n",
        "Para ello, varios de sus algoritmos implementan cambios que consideren la posibilidad de que haya múltiples entornos subyacentes, por ejemplo `step(accion)` cambia a `step(lista_acciones)`, aplicando acciones a todos los entornos, recibiendo ahora múltiples observaciones y recompensas.\n",
        "\n",
        "Otro cambio: se aplica `reset()` automáticamente a cada entorno que llega a un estado final."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GGZjjcDoIJN"
      },
      "source": [
        "SB brinda dos formas de utilizar entornos vectorizados:\n",
        "\n",
        "* **DummyVecEnv**, el cuál consiste en un *wrapper* de varios entornos, los cuáles funcionarán en un sólo hilo. Este wrapper es útil como entrada de algoritmos que requieren los entornos de esta forma, y habilita los procesamientos y operaciones comunes de los entornos vectorizados.\n",
        "* **SubprocVecEnv**, el cuál paraleliza multiples entornos pero en procesos de ejecucíon separados. Cada proceso tiene su propia memoria y puede adquirir derechos sobre las CPUs de la computadora donde se ejecuta. Se utiliza cuando el entorno del agente es computacionalemente complejo. Atención! **Puede comer mucha RAM**.\n",
        "\n",
        "Vemos un ejemplo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovc6JDmYoIJN",
        "outputId": "177a956d-a63f-4c13-c049-5c1c1f32a0d1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<stable_baselines3.ppo.ppo.PPO at 0x7f780a7d32b0>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# ejemplo de ambiente dummy\n",
        "venv = DummyVecEnv([lambda: gym.make('CartPole-v1')]*4)\n",
        "\n",
        "model = PPO('MlpPolicy', venv, )\n",
        "model.learn(total_timesteps=10000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkadM0S1oIJO"
      },
      "source": [
        "También puede hacerse con un una función de SB a tal efecto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wez-ZY_IoIJO",
        "outputId": "d8a1d565-c8ba-476a-90e1-77c20f46c485"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<stable_baselines3.ppo.ppo.PPO at 0x7f780a809d30>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "venv = make_vec_env(lambda: env, n_envs=4)\n",
        "\n",
        "model = PPO('MlpPolicy', venv, )\n",
        "model.learn(total_timesteps=10000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjH4ZQ4NoIJO"
      },
      "source": [
        "### Ejecutar agente con políticas personalizadas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jp_aPhIdoIJO",
        "outputId": "318e9dd8-8098-49a1-ff0d-cb23887c4a45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n",
            "Creating environment from the given name 'CartPole-v1'\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 20.6     |\n",
            "|    ep_rew_mean     | 20.6     |\n",
            "| time/              |          |\n",
            "|    fps             | 1266     |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 1        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 25.4        |\n",
            "|    ep_rew_mean          | 25.4        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 692         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 5           |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008125117 |\n",
            "|    clip_fraction        | 0.0909      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.686      |\n",
            "|    explained_variance   | -0.000661   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 3.54        |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.00851    |\n",
            "|    value_loss           | 35.1        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 32.7        |\n",
            "|    ep_rew_mean          | 32.7        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 655         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 9           |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011632612 |\n",
            "|    clip_fraction        | 0.152       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.667      |\n",
            "|    explained_variance   | 0.27        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 8.42        |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.0165     |\n",
            "|    value_loss           | 26.8        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 44.7       |\n",
            "|    ep_rew_mean          | 44.7       |\n",
            "| time/                   |            |\n",
            "|    fps                  | 689        |\n",
            "|    iterations           | 4          |\n",
            "|    time_elapsed         | 11         |\n",
            "|    total_timesteps      | 8192       |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02182687 |\n",
            "|    clip_fraction        | 0.198      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.633     |\n",
            "|    explained_variance   | 0.334      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 22.7       |\n",
            "|    n_updates            | 30         |\n",
            "|    policy_gradient_loss | -0.018     |\n",
            "|    value_loss           | 44.5       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 61.3        |\n",
            "|    ep_rew_mean          | 61.3        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 715         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 14          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021517992 |\n",
            "|    clip_fraction        | 0.0658      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.607      |\n",
            "|    explained_variance   | 0.301       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 23.1        |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.00758    |\n",
            "|    value_loss           | 61.4        |\n",
            "-----------------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<stable_baselines3.ppo.ppo.PPO at 0x7f780a7d3ee0>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# Creamos una clase con una red neuronal de 128x128 neuronas\n",
        "\n",
        "model = PPO('MlpPolicy', policy_kwargs=dict(net_arch=[128,128]), env='CartPole-v1', verbose=1)\n",
        "model.learn(total_timesteps=10000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pGjTA2X6hFO"
      },
      "source": [
        "[Buen post](https://medium.com/aureliantactics/understanding-ppo-plots-in-tensorboard-cbc3199b9ba2) en donde se explica qué significan varias de estas métricas. Para verlas en detalle podemos consultar directamente el [código fuente](https://github.com/DLR-RM/stable-baselines3/blob/master/stable_baselines3/ppo/ppo.py), que está bien documentado y no es muy difícil de seguir."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbwkBi87oIJO"
      },
      "source": [
        "### Utilizar un entorno personalizado\n",
        "\n",
        "Antes que nada, además de la interfaz que ya vimos de Gym, hay otras nociones que tenemos que tener en cuenta en este contexto:\n",
        "\n",
        "* Los entornos definen un espacio de estados y de acciones, a partir de los cuáles los modelos asumen y respetan la \"forma\" de observaciones y acciones. Por ejemplo, algunos algoritmos están diseñados para espacios de acciones discretos (DQN), continuos (DDPG) o bien poseen implementaciones particulares pueden usarse en ambos (PPO, en el repo de SB3). En cuanto a los espacios, algunos algoritmos asumen explícitamente un espacio discreto (y pequeño), como Q-Learning, mientras que otros como PPO asumen cualquier tipo de espacio.\n",
        "* Los dos tipos más comunes de estados o acciones son los espacios discretos `gym.spaces.Discrete` y los continuos `gym.spaces.Box`.\n",
        "* Los espacios discretos definen un conjunto de $n$ estados/acciones $\\{ 0, 1, \\dots, n-1 \\}$, mientras que los espacios continuos definen un espacio $\\mathbb{R}^d$, de una de las siguientes 4 formas: $[a, b], (-\\infty, b], [a, \\infty), (-\\infty, \\infty)$, en donde $a,b$ son las cotas superior e inferior (de existir).\n",
        "* Ejemplos: un espacio de acciones `Discrete(4)` tiene 4 acciones: $\\{0,1,2,3\\}$; un espacio de estados `Discrete(16)` tiene 16 estados. Un espacio de estados ALTURA, ANCHO, N_CANALES que represente una imagen RGB acotada en $[a=0, b=255]$ se puede crear como\n",
        "\n",
        "`observation_space = spaces.Box(low=0, high=255, shape=(HEIGHT, WIDTH, N_CHANNELS), dtype=np.uint8)`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFLZieLNoIJO"
      },
      "source": [
        "Para usar un entorno compatible por esta librería, el mismo tiene que heredar de *gym.Env*. Vemos un ejemplo (crédito: Antonin Raffin https://colab.research.google.com/github/araffin/rl-tutorial-jnrr19/blob/sb3/5_custom_gym_env.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "str0v6DUoIJO"
      },
      "outputs": [],
      "source": [
        "class GoLeftEnv(gym.Env):\n",
        "  #clase heredada de los ambientes de gym\n",
        "  \"\"\"\n",
        "  Ambiente personalizado que sigue la interfaz de gym.\n",
        "  Es un entorno simple en el cuál el agente debe aprender a ir siempre \n",
        "  hacia la izquierda.\n",
        "  \"\"\"\n",
        "  # Dado que podemos estar en colab, no podemos implementar la salida por interfaz \n",
        "  # gráfica ('human' render mode) \n",
        "  metadata = {'render.modes': ['console']}\n",
        "  # Definimos las constantes\n",
        "  LEFT = 0\n",
        "  RIGHT = 1\n",
        "\n",
        "  def __init__(self, grid_size=10):\n",
        "    super(GoLeftEnv, self).__init__()\n",
        "\n",
        "    # Tamaño de la grilla de 1D\n",
        "    self.grid_size = grid_size\n",
        "    # Inicializamos en agente a la derecha de la grilla\n",
        "    self.agent_pos = grid_size - 1\n",
        "\n",
        "    # Definimos el espacio de acción y observaciones\n",
        "    # Los mismos deben ser objetos gym.spaces\n",
        "    # En este ejemplo usamos dos acciones discretas: izquierda y derecha\n",
        "    n_actions = 2\n",
        "    self.action_space = spaces.Discrete(n_actions)\n",
        "    # La observación será la coordenada donde se encuentra el agente\n",
        "    # puede ser descrita tanto por los espacios Discrete como Box\n",
        "    self.observation_space = spaces.Box(low=0, high=self.grid_size,\n",
        "                                        shape=(2,), dtype=np.float32)\n",
        "\n",
        "  def reset(self):\n",
        "    \"\"\"\n",
        "    Importante: la observación devuelta debe ser un array de numpy\n",
        "    :return: (np.array) \n",
        "    \"\"\"\n",
        "    # Se inicializa el agente a la derecha de la grilla\n",
        "    self.agent_pos = self.grid_size - 1\n",
        "    # convertimos con astype a float32 (numpy) para hacer más general el agente\n",
        "    # (en caso de que querramos usar acciones continuas)\n",
        "    return np.array([self.agent_pos]).astype(np.float32)\n",
        "\n",
        "  def step(self, action):\n",
        "    if action == self.LEFT:\n",
        "      self.agent_pos -= 1\n",
        "    elif action == self.RIGHT:\n",
        "      self.agent_pos += 1\n",
        "    else:\n",
        "      raise ValueError(\"Se recibió una acción inválida={} que no es parte del espacio de acciones\".format(action))\n",
        "\n",
        "    # Evitamos que el agente se salga de los límites de la grilla\n",
        "    self.agent_pos = np.clip(self.agent_pos, 0, self.grid_size)\n",
        "\n",
        "    # Llegó el agente a su estado objetivo (izquierda) de la grilla?\n",
        "    done = bool(self.agent_pos == 0)\n",
        "\n",
        "    # Asignamos recompensa sólo cuando el agente llega a su objetivo\n",
        "    # (recompensa = 0 en todos los demás estados)\n",
        "    reward = 1 if self.agent_pos == 0 else 0\n",
        "\n",
        "    # gym también nos permite devolver información adicional, ej. en Atari: \n",
        "    # las vidas restantes del agente (no usaremos esto por ahora)\n",
        "    info = {}\n",
        "\n",
        "    return np.array([self.agent_pos]).astype(np.float32), reward, done, info\n",
        "\n",
        "  def render(self, mode='console'):\n",
        "    if mode != 'console':\n",
        "      raise NotImplementedError()\n",
        "    # en nuestra interfaz de consola, representamos el agente como una cruz, y \n",
        "    # el resto como un punto\n",
        "    print(\".\" * self.agent_pos, end=\"\")\n",
        "    print(\"x\", end=\"\")\n",
        "    print(\".\" * (self.grid_size - self.agent_pos))\n",
        "\n",
        "  def close(self):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flB9-j1SoIJP",
        "outputId": "d80c13fb-d15d-477c-ccde-89db4ef83b1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 114      |\n",
            "|    ep_rew_mean      | 1        |\n",
            "|    exploration_rate | 0.783    |\n",
            "| time/               |          |\n",
            "|    episodes         | 4        |\n",
            "|    fps              | 8553     |\n",
            "|    time_elapsed     | 0        |\n",
            "|    total_timesteps  | 457      |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 134      |\n",
            "|    ep_rew_mean      | 1        |\n",
            "|    exploration_rate | 0.491    |\n",
            "| time/               |          |\n",
            "|    episodes         | 8        |\n",
            "|    fps              | 9085     |\n",
            "|    time_elapsed     | 0        |\n",
            "|    total_timesteps  | 1071     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 110      |\n",
            "|    ep_rew_mean      | 1        |\n",
            "|    exploration_rate | 0.372    |\n",
            "| time/               |          |\n",
            "|    episodes         | 12       |\n",
            "|    fps              | 8802     |\n",
            "|    time_elapsed     | 0        |\n",
            "|    total_timesteps  | 1323     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 103      |\n",
            "|    ep_rew_mean      | 1        |\n",
            "|    exploration_rate | 0.217    |\n",
            "| time/               |          |\n",
            "|    episodes         | 16       |\n",
            "|    fps              | 8616     |\n",
            "|    time_elapsed     | 0        |\n",
            "|    total_timesteps  | 1649     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 93.7     |\n",
            "|    ep_rew_mean      | 1        |\n",
            "|    exploration_rate | 0.11     |\n",
            "| time/               |          |\n",
            "|    episodes         | 20       |\n",
            "|    fps              | 8382     |\n",
            "|    time_elapsed     | 0        |\n",
            "|    total_timesteps  | 1874     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 94.4     |\n",
            "|    ep_rew_mean      | 1        |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 24       |\n",
            "|    fps              | 8435     |\n",
            "|    time_elapsed     | 0        |\n",
            "|    total_timesteps  | 2265     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 89.3     |\n",
            "|    ep_rew_mean      | 1        |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 28       |\n",
            "|    fps              | 8423     |\n",
            "|    time_elapsed     | 0        |\n",
            "|    total_timesteps  | 2500     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 93.9     |\n",
            "|    ep_rew_mean      | 1        |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 32       |\n",
            "|    fps              | 8529     |\n",
            "|    time_elapsed     | 0        |\n",
            "|    total_timesteps  | 3006     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 95.9     |\n",
            "|    ep_rew_mean      | 1        |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 36       |\n",
            "|    fps              | 8504     |\n",
            "|    time_elapsed     | 0        |\n",
            "|    total_timesteps  | 3452     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 109      |\n",
            "|    ep_rew_mean      | 1        |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 40       |\n",
            "|    fps              | 8439     |\n",
            "|    time_elapsed     | 0        |\n",
            "|    total_timesteps  | 4346     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 126      |\n",
            "|    ep_rew_mean      | 1        |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 44       |\n",
            "|    fps              | 8488     |\n",
            "|    time_elapsed     | 0        |\n",
            "|    total_timesteps  | 5552     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 129      |\n",
            "|    ep_rew_mean      | 1        |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 48       |\n",
            "|    fps              | 8561     |\n",
            "|    time_elapsed     | 0        |\n",
            "|    total_timesteps  | 6173     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 133      |\n",
            "|    ep_rew_mean      | 1        |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 52       |\n",
            "|    fps              | 8665     |\n",
            "|    time_elapsed     | 0        |\n",
            "|    total_timesteps  | 6898     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 130      |\n",
            "|    ep_rew_mean      | 1        |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 56       |\n",
            "|    fps              | 8532     |\n",
            "|    time_elapsed     | 0        |\n",
            "|    total_timesteps  | 7299     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 131      |\n",
            "|    ep_rew_mean      | 1        |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 60       |\n",
            "|    fps              | 8572     |\n",
            "|    time_elapsed     | 0        |\n",
            "|    total_timesteps  | 7855     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 127      |\n",
            "|    ep_rew_mean      | 1        |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 64       |\n",
            "|    fps              | 8545     |\n",
            "|    time_elapsed     | 0        |\n",
            "|    total_timesteps  | 8101     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 128      |\n",
            "|    ep_rew_mean      | 1        |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 68       |\n",
            "|    fps              | 8531     |\n",
            "|    time_elapsed     | 1        |\n",
            "|    total_timesteps  | 8686     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 126      |\n",
            "|    ep_rew_mean      | 1        |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 72       |\n",
            "|    fps              | 8533     |\n",
            "|    time_elapsed     | 1        |\n",
            "|    total_timesteps  | 9049     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 125      |\n",
            "|    ep_rew_mean      | 1        |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 76       |\n",
            "|    fps              | 8556     |\n",
            "|    time_elapsed     | 1        |\n",
            "|    total_timesteps  | 9486     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 129      |\n",
            "|    ep_rew_mean      | 1        |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 80       |\n",
            "|    fps              | 8505     |\n",
            "|    time_elapsed     | 1        |\n",
            "|    total_timesteps  | 10291    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 127      |\n",
            "|    ep_rew_mean      | 1        |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 84       |\n",
            "|    fps              | 8435     |\n",
            "|    time_elapsed     | 1        |\n",
            "|    total_timesteps  | 10668    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 127      |\n",
            "|    ep_rew_mean      | 1        |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 88       |\n",
            "|    fps              | 8369     |\n",
            "|    time_elapsed     | 1        |\n",
            "|    total_timesteps  | 11151    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 127      |\n",
            "|    ep_rew_mean      | 1        |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 92       |\n",
            "|    fps              | 8408     |\n",
            "|    time_elapsed     | 1        |\n",
            "|    total_timesteps  | 11666    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 123      |\n",
            "|    ep_rew_mean      | 1        |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 96       |\n",
            "|    fps              | 8381     |\n",
            "|    time_elapsed     | 1        |\n",
            "|    total_timesteps  | 11851    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 122      |\n",
            "|    ep_rew_mean      | 1        |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 100      |\n",
            "|    fps              | 8370     |\n",
            "|    time_elapsed     | 1        |\n",
            "|    total_timesteps  | 12231    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 123      |\n",
            "|    ep_rew_mean      | 1        |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 104      |\n",
            "|    fps              | 8329     |\n",
            "|    time_elapsed     | 1        |\n",
            "|    total_timesteps  | 12717    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 121      |\n",
            "|    ep_rew_mean      | 1        |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 108      |\n",
            "|    fps              | 8349     |\n",
            "|    time_elapsed     | 1        |\n",
            "|    total_timesteps  | 13146    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 121      |\n",
            "|    ep_rew_mean      | 1        |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 112      |\n",
            "|    fps              | 8344     |\n",
            "|    time_elapsed     | 1        |\n",
            "|    total_timesteps  | 13392    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 120      |\n",
            "|    ep_rew_mean      | 1        |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 116      |\n",
            "|    fps              | 8331     |\n",
            "|    time_elapsed     | 1        |\n",
            "|    total_timesteps  | 13663    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 121      |\n",
            "|    ep_rew_mean      | 1        |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 120      |\n",
            "|    fps              | 8320     |\n",
            "|    time_elapsed     | 1        |\n",
            "|    total_timesteps  | 13934    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 119      |\n",
            "|    ep_rew_mean      | 1        |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 124      |\n",
            "|    fps              | 8314     |\n",
            "|    time_elapsed     | 1        |\n",
            "|    total_timesteps  | 14166    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 119      |\n",
            "|    ep_rew_mean      | 1        |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 128      |\n",
            "|    fps              | 8300     |\n",
            "|    time_elapsed     | 1        |\n",
            "|    total_timesteps  | 14362    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 116      |\n",
            "|    ep_rew_mean      | 1        |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 132      |\n",
            "|    fps              | 8308     |\n",
            "|    time_elapsed     | 1        |\n",
            "|    total_timesteps  | 14614    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 116      |\n",
            "|    ep_rew_mean      | 1        |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 136      |\n",
            "|    fps              | 8290     |\n",
            "|    time_elapsed     | 1        |\n",
            "|    total_timesteps  | 15032    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 111      |\n",
            "|    ep_rew_mean      | 1        |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 140      |\n",
            "|    fps              | 8285     |\n",
            "|    time_elapsed     | 1        |\n",
            "|    total_timesteps  | 15402    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 103      |\n",
            "|    ep_rew_mean      | 1        |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 144      |\n",
            "|    fps              | 8146     |\n",
            "|    time_elapsed     | 1        |\n",
            "|    total_timesteps  | 15822    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 98.8     |\n",
            "|    ep_rew_mean      | 1        |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 148      |\n",
            "|    fps              | 8123     |\n",
            "|    time_elapsed     | 1        |\n",
            "|    total_timesteps  | 16054    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 94.7     |\n",
            "|    ep_rew_mean      | 1        |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 152      |\n",
            "|    fps              | 8129     |\n",
            "|    time_elapsed     | 2        |\n",
            "|    total_timesteps  | 16369    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 93       |\n",
            "|    ep_rew_mean      | 1        |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 156      |\n",
            "|    fps              | 8116     |\n",
            "|    time_elapsed     | 2        |\n",
            "|    total_timesteps  | 16601    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 90       |\n",
            "|    ep_rew_mean      | 1        |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 160      |\n",
            "|    fps              | 8117     |\n",
            "|    time_elapsed     | 2        |\n",
            "|    total_timesteps  | 16855    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 91.1     |\n",
            "|    ep_rew_mean      | 1        |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 164      |\n",
            "|    fps              | 8129     |\n",
            "|    time_elapsed     | 2        |\n",
            "|    total_timesteps  | 17210    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 90       |\n",
            "|    ep_rew_mean      | 1        |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 168      |\n",
            "|    fps              | 8111     |\n",
            "|    time_elapsed     | 2        |\n",
            "|    total_timesteps  | 17690    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 92.2     |\n",
            "|    ep_rew_mean      | 1        |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 172      |\n",
            "|    fps              | 8142     |\n",
            "|    time_elapsed     | 2        |\n",
            "|    total_timesteps  | 18266    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 92.2     |\n",
            "|    ep_rew_mean      | 1        |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 176      |\n",
            "|    fps              | 8133     |\n",
            "|    time_elapsed     | 2        |\n",
            "|    total_timesteps  | 18706    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 89.2     |\n",
            "|    ep_rew_mean      | 1        |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 180      |\n",
            "|    fps              | 8153     |\n",
            "|    time_elapsed     | 2        |\n",
            "|    total_timesteps  | 19214    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 88.2     |\n",
            "|    ep_rew_mean      | 1        |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 184      |\n",
            "|    fps              | 8157     |\n",
            "|    time_elapsed     | 2        |\n",
            "|    total_timesteps  | 19486    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 85.5     |\n",
            "|    ep_rew_mean      | 1        |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 188      |\n",
            "|    fps              | 8152     |\n",
            "|    time_elapsed     | 2        |\n",
            "|    total_timesteps  | 19702    |\n",
            "----------------------------------\n"
          ]
        }
      ],
      "source": [
        "env = GoLeftEnv(grid_size=10)\n",
        "env = make_vec_env(lambda: env, n_envs=1)\n",
        "\n",
        "model = DQN('MlpPolicy', env, verbose=1).learn(20000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VC5TczTL6hFT"
      },
      "source": [
        "Ver agente entrenado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-OcrgC36hFU",
        "outputId": "ce022098-d783-4dec-be72-a5fae69e5432"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "........x..\n",
            ".......x...\n",
            "......x....\n",
            ".....x.....\n",
            "....x......\n",
            "...x.......\n",
            "..x........\n",
            ".x.........\n",
            ".........x.\n",
            "........x..\n"
          ]
        }
      ],
      "source": [
        "obs = env.reset()\n",
        "for i in range(10):\n",
        "    action, _states = model.predict(obs)\n",
        "    obs, rewards, dones, info = env.step(action)\n",
        "    env.render(mode='console')\n",
        "    if dones[0]:\n",
        "        obs = env.reset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgrWKJlsoIJT"
      },
      "source": [
        "## Normalización de features y recompensas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIfn3o99oIJT"
      },
      "source": [
        "Stable-Baselines3 tiene predefinidos un conjunto de **wrappers** genericos que pueden utilizarse para preprocesar las observaciones que llegan al agente RL, desacoplando del mismo el prepocesamiento.\n",
        "\n",
        "Entre las funcionalidades disponibles tenemos:\n",
        "* **VecFrameStack**: Se utiliza cuando la observación que percibe el agente es una imagen. Sirve para expandir el espacio de estados apilando N frames de manera conjunta (ver [paper de DQN](https://arxiv.org/abs/1312.5602) para más detalle).\n",
        "* **VecNormalize**: Se utiliza para normalizar las observaciónes y/o las recompenzas que percibe el agente, a $\\mu=0$ y $\\sigma=1$. También permite cortar valores de observaciones y/o recompensas que excedan un rango establecido. \n",
        "* **VecCheckNan**: Se utiliza para trackear los estados del entorno que generan que los gradientes de la RNN se hagan NaN.\n",
        "* **VecVideoRecorder**: Se utiliza para exportar el funcionamiento de la política aprendida por el agente a un video (MP4).\n",
        "\n",
        "Ademas, se pueden crear **wrappers** personalizados extendiendo la clase **VecEnvWrapper**:\n",
        "\n",
        "```\n",
        "class MiWrapper(VecEnvWrapper):\n",
        "    [...]\n",
        "```\n",
        "### Ejemplo (VecNormalize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VOh1HQCVJNA",
        "outputId": "9039731b-68f0-41cf-b607-011817f76c5c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<stable_baselines3.dqn.dqn.DQN at 0x7f780a718a30>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
        "\n",
        "env = gym.make('CartPole-v1')\n",
        "env = DummyVecEnv([lambda: env])  # Multiples entornos vectorizados\n",
        "# Normalizamos tanto observaciones como recompensas, y recortamos ambas cuando exceden 10\n",
        "env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.0, clip_reward=10.0)\n",
        "\n",
        "model = DQN('MlpPolicy', env)\n",
        "model.learn(total_timesteps=10000)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exel4Gr8oIJQ"
      },
      "source": [
        "# RL-baselines3-zoo\n",
        "\n",
        "Colección de agentes RL y herramientas útiles para ejecutarlos, evaluarlos e incluso hacer videos con ellos. Los agentes de este repo están preparados con la configuración requerida para los distintos tipos de entornos, incluyendo Atari, PyBullet y entornos clásicos, incluyendo configuraciones e híper-parámetros que producen buenas políticas para tales entornos.\n",
        "\n",
        "Esta librería ofrece un muy buen punto de partida para utilizar agentes / entornos personalizados, ya que ofrece una [interfaz](https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/train.py) fácilmente adaptable a nuestras necesidades.\n",
        "\n",
        "Si se usan entornos personalizados con rl-baselines3-zoo, debe tenerse en cuenta que se deben definir todos los híper-parámetros de antemano, sea al instanciar el agente, o en la carpeta */rl-baselines3-zoo/hyperparams*; de lo contrario arrojará error por no encontrar qué híper-parámetro usar.\n",
        "\n",
        "**Importante:** esta librería es enteramente opcional, no es necesaria para realizar los labs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dVRDazhO6hFa"
      },
      "outputs": [],
      "source": [
        "# verificamos si está el repo rl-baselines3-zoo, si es así ejecutamos las celdas siguientes\n",
        "repo_downloaded = os.path.exists('rl-baselines3-zoo')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6TCyu_UoIJR"
      },
      "source": [
        "## Ejecución\n",
        "\n",
        "Los agentes pueden ser llamados desde la consola mediante comandos como\n",
        "\n",
        "`python train.py --algo algo_name --env env_id`\n",
        "\n",
        "Los cuales pueden ser llamados usando"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jBLMVxhIoIJR"
      },
      "outputs": [],
      "source": [
        "if repo_downloaded:\n",
        "\n",
        "    os.chdir('rl-baselines3-zoo/')\n",
        "\n",
        "    args = [\n",
        "        '-n', str(100000),\n",
        "        '--algo', 'ppo',\n",
        "        '--env', 'CartPole-v1'\n",
        "    ]\n",
        "\n",
        "    p = Popen(['python', 'train.py'] + args,\n",
        "                                   stdin=PIPE, stdout=PIPE, stderr=PIPE)\n",
        "    output, err = p.communicate()\n",
        "    rc = p.returncode\n",
        "    os.chdir(cwd)\n",
        "    assert rc == 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40H4hNcqoIJR"
      },
      "source": [
        "Ver en acción el agente entrenado (nota: no disponible en Google Colab, requiere ffmpeg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNija96loIJR"
      },
      "outputs": [],
      "source": [
        "if not IN_COLAB and repo_downloaded:\n",
        "    os.chdir('rl-baselines3-zoo/')\n",
        "\n",
        "    args = [\n",
        "        '--algo', 'ppo',\n",
        "        '--env', 'CartPole-v1',\n",
        "        '--folder', 'logs/'\n",
        "    ]\n",
        "\n",
        "    p = Popen(['python', 'enjoy.py'] + args,\n",
        "                                   stdin=PIPE, stdout=PIPE, stderr=PIPE)\n",
        "    output, err = p.communicate()\n",
        "    rc = p.returncode\n",
        "    os.chdir(cwd)\n",
        "    assert rc == 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSfIitK7oIJR"
      },
      "source": [
        "También es posible grabar un video! Ver https://stable-baselines3.readthedocs.io/en/master/guide/examples.html#record-a-video"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Phq975BYoIJR"
      },
      "source": [
        "Ver curva de aprendizaje obtenida por el agente desde *utils.plot*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8EA41oQoIJS"
      },
      "outputs": [],
      "source": [
        "os.chdir('rl-baselines3-zoo/')\n",
        "\n",
        "args = [\n",
        "    '--algo', 'ppo',\n",
        "    '--env', 'CartPole-v1',\n",
        "    '--exp-folder', 'logs/'\n",
        "]\n",
        "\n",
        "p = Popen(['python', '-m', 'scripts.plot_train'] + args, stdout=PIPE)\n",
        "output, err = p.communicate()\n",
        "rc = p.returncode\n",
        "os.chdir(cwd)\n",
        "\n",
        "assert rc == 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__1mgFR0oIJT",
        "outputId": "3f4664ed-28a3-407b-a858-25ccae1b2083"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'Figure(640x480)\\n'\n"
          ]
        }
      ],
      "source": [
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWzAHj6qoIJT"
      },
      "source": [
        "## Híper-parámetros\n",
        "\n",
        "rl-baselines-zoo provee híper-parámetros que resultan en curvas de aprendizaje que convergen en buena cantidad de entornos. Estos híper-parámetros pueden verse en cada uno de los archivos YAML de cada algoritmo, [acá](https://github.com/DLR-RM/rl-baselines3-zoo/tree/master/hyperparams).\n",
        "\n",
        "También provee funcionalidad para optimizar los híper-parámetros con la librería [Optuna]( https://github.com/optuna/optuna). En los mismos se incluyen rangos de híper-parámetros que se usaron para optimizar entornos como los de PyBullet, y son fácilmente modificables para adaptarlo a nuestros propios entornos. Para ver cómo se llama a la interfaz de Optuna ver [este código](https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/utils/hyperparams_opt.py).\n",
        "\n",
        "Nota: **Optuna come muchos recursos!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fjfDHkh6hFm"
      },
      "source": [
        "# Entrenando otros agentes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrZQ_iUU6hFm"
      },
      "source": [
        "### Algunos conjuntos de entornos\n",
        "\n",
        "CartPole es una excelente línea base (de hecho suele ser la prueba preliminar de todo nuevo algoritmo), porque tiene recompensas constantes pero requiere cierta solidez por parte del algoritmo para hacerlo converger al óptimo.\n",
        "\n",
        "No obstante, al implementar un algoritmo, es deseable que el mismo pueda desenvolverse de forma consistente en varios grupos de entornos. A continuación va una lista con varios entornos que sirven como prueba:\n",
        "\n",
        "| Entornos                                                                                                           | Estados            | Acciones            | Dificultad      | Implementado por                                                                                                                                                            |\n",
        "|--------------------------------------------------------------------------------------------------------------------|--------------------|---------------------|-----------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
        "| Clásicos (CartPole, MountainCar, Pendulum, Deep sea (testea exploración), umbrella (testea asignación de crédito)) | Discretos/Continuos          | Continuas/discretas | Baja/Media      | [Gym](https://github.com/openai/gym/wiki/Table-of-environments) y [BSuite](https://github.com/deepmind/bsuite)                                                                                                                          |\n",
        "| Grilla (desde pequeñas donde hay que salir hasta grandes con muchas habitaciones y subproblemas)                   | Discretos/imágenes | Discretas           | Baja/Media/Alta | [gym-minigrid](https://github.com/maximecb/gym-minigrid)                                                                                                                    |\n",
        "| Grilla/plataforma/estilo Atari, generados proceduralmente                                                          | Imágenes           | Discretas           | Media/Alta      | [Gym](https://github.com/openai/procgen)                                                                                                                                    |\n",
        "| Plataforma 2D, como LunarLander o BipedalWalker                                                                    | Continuos          | Continuas/discretas | Media/Alta      | [Gym](https://github.com/openai/gym/wiki/Table-of-environments)                                                                                                                                                                         |\n",
        "| Primera persona en 3D                                                                                              | Imágenes           | Continuas/discretas | Media/Alta      | [Deepmind](https://github.com/deepmind/lab)                                                                                                                                 |\n",
        "| Simulación física de pequeños robots                                                                               | Continuos          | Continuas           | Media/Alta      | Gym (con el motor [MuJoCo](https://mujoco.org/) o [PyBullet](https://docs.google.com/document/d/10sXEhzFRSnvFcl3XxNGhnD4N2SedqwdAvK3dsihxVUA/edit#heading=h.wz5to0x8kqmr)) |\n",
        "| Atari                                                                                                              | Continuos          | Discretas           | Media/Alta      | [Gym](https://github.com/openai/gym/wiki/Table-of-environments) (son todos aquellos entornos que terminan en \"*-v4*\"                                                                                                                                                                         |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oefsbUw86hFn"
      },
      "source": [
        "## Resumen de algunos algoritmos\n",
        "\n",
        "Se resumen ahora varios algoritmos del estado del arte de aprendizaje por refuerzos profundo\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3engGbd6hFo"
      },
      "source": [
        "| Algoritmo | Tipo       | Espacio de acciones | Resumen rápido                                                                                                                                                | Artículo                         |\n",
        "|-----------|------------|---------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------|\n",
        "| DQN       | Off-policy | Discretas           | Extiende Q-Learning a deep learning. En Stable-baselines, DQN incluye todas las mejoras ya incorporadas.                                                      | https://arxiv.org/abs/1312.5602  |\n",
        "| ACER      | Off-policy | Discretas           | Combina una arquitectura actor-critic con un buffer y repetición de experiencia.                                                                              | https://arxiv.org/abs/1611.01224 |\n",
        "| A3C       | On-policy  | Ambos               | Múltiples agentes corriendo en múltiples instancias del ambiente, acumulando sus gradientes y actualizándolos tras un cierto tiempo.                          | https://arxiv.org/abs/1602.01783 |\n",
        "| PPO       | On-policy  | Ambos               | Punta de lanza de policy gradient, incluye mecanismo para que el gradiente actualice de forma acotada, mejorando drásticamente la estabilidad.             | https://arxiv.org/abs/1707.06347 |\n",
        "| DDPG      | Off-policy | Continuas           | Como en espacios de acciones continuos es muy difícil encontrar $\\max_a Q(s,a)$, se aproxima via $Q(s, a(s \\mid \\theta_a))$, siendo $a$ un actor estocástico. | https://arxiv.org/abs/1509.02971 |\n",
        "| TD3       | Off-policy | Continuas           | Mejora DDPG utilizando dos funciones $Q$ y retrasando la actualización para reducir la sobreestimación de $Q$.                                                | https://arxiv.org/abs/1802.09477 |\n",
        "| SAC       | Off-policy | Continuas           | Usa dos funciones $Q$, introduce el bonus por entropía y usa un actor estocástico que muestrea acciones según una política $\\pi$.                             | https://arxiv.org/abs/1801.01290 |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYnYDR7w6hFo"
      },
      "source": [
        "## Resumen de algunas herramientas/trucos comúnmente usados\n",
        "\n",
        "* Experience replay/buffer de experiencia: guarda las experiencias en un buffer para poder usarlas repetidamente durante el entrenamiento.\n",
        "    * Ventajas: experiencias raras pero muy relevantes (ej: que tienen mucho error de actualización) quedan guardadas en memoria, pudiendo ser usadas repetidamente para aprender sin necesidad de esperar a que se repitan.\n",
        "    \n",
        "    En métodos de gradiente de política, en cambio, el aprendizaje queda reflejado en los pesos, lo cuál puede hacer que un agente no se desenvuelva correctamente en entornos de recompensa escasa como MountainCar.\n",
        "    \n",
        "    * Desventajas: requiere considerable RAM, son usables solamente en algoritmos off-policy y su muestreo no necesariamente refleja la probabilidad real de tener esas experiencias en el entorno (añadiendo sesgo), por lo que es recomendable usarlo junto con importance sampling.\n",
        "    \n",
        "* Importance sampling: aplica un descuento a las actualizaciones a partir de experience replay relacionado a cuán probable era realizar esa transición.\n",
        "* Entropía: añade un bonus a la función de recompensa para que bonifique políticas $\\pi(s \\mid a)$ que tengan mayor entropía que otras, motivando la exploración del agente.\n",
        "* Juntar varias secuencias de imágenes. Usado principalmente en entornos de Atari para poder evaluar la dirección de movimientos.\n",
        "* Normalización de recompensas/estados. Normaliza las recompensas y observaciones usualmente con una media móvil, de modo tal que las observaciones/recompensas reflejen su relación y varianza con respecto a las demás.\n",
        "* Clipping (recorte) de recompensas. Se usaba principalmente en entornos de Atari con algoritmos tipo DQN para recortar el impacto que las distintas recompensas tenían, a una constante (ej: 1). Suele usarse como una cota máxima de recompensas/observaciones normalizadas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUVThVCZ6hFp"
      },
      "source": [
        "# Lab 2\n",
        "\n",
        "1. Crear tu propio entorno y entrenar agentes RL en el mismo. Analizar la convergencia con distintos algoritmos* (ej: PPO, DQN), resultados con distintas funciones de recompensa e híper-parámetros. \n",
        "\n",
        "    Algunas ideas:\n",
        "\n",
        "    * Transformar GoLeftEnv en una grilla 2D, añadir paredes / trampas / agua.\n",
        "    * Crear un entorno que juegue a algún juego como el ta-te-ti.\n",
        "    * Crea un entorno totalmente nuevo que sea de tu interés!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwXMoJy9oIJT"
      },
      "source": [
        "## **Snake clásico modififcado**\n",
        "\n",
        "Implementamos una modificación al juego de arcade clásico Snake, agregando paredes. En este juego una serpiente se mueve por un tablero y debe recoger frutas para comer. Utilizamos el algoritmo \"PP0\" como técnica de aprendizaje.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fCMS5pYBls5p"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "from gym import spaces\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "class Serpiente(gym.Env):\n",
        "    \"\"\"\n",
        "    Entorno personalizado para Stable Baseline 3 para el clásico Snake \n",
        "    con modificación de paredes\n",
        "    \"\"\"\n",
        "    metadata = {'render.modes': ['console','rgb_array']}\n",
        "    #Direction constants\n",
        "    n_actions = 3 #3 posibles pasos por cada turno\n",
        "    LEFT = 0\n",
        "    STRAIGHT = 1\n",
        "    RIGHT = 2\n",
        "    #Grid label constants\n",
        "    EMPTY = 0\n",
        "    SNAKE = 1\n",
        "    WALL = 2\n",
        "    FOOD = 3\n",
        "    #Rewards\n",
        "    REWARD_PER_STEP = 0 # recompensa por cada paso dado, entra en bucles infinitos si >0\n",
        "    #Definir pasos máximos para evitar bucles infinitos\n",
        "    REWARD_WALL_HIT = -20 #debe ser inferior a -REWARD_PER_STEP_TOWARDS_FOOD para evitar golpear la pared intencionalmente\n",
        "    REWARD_PER_STEP_TOWARDS_FOOD = 1 #dar recompensa por moverse hacia la comida y penalización por alejarse\n",
        "    REWARD_PER_FOOD = 100 \n",
        "    MAX_STEPS_AFTER_FOOD = 200 #parar si pasamos demasiado tiempo sin comer para evitar bucles infinitos\n",
        "\n",
        "\n",
        "    def __init__(self, grid_size=6):\n",
        "        super(Serpiente, self).__init__()\n",
        "        #Pasos\n",
        "        self.stepnum = 0; self.last_food_step=0\n",
        "        # Tamaño de la cuadrícula 2D (incluidas las paredes)\n",
        "        self.grid_size = grid_size\n",
        "        # Inicializar la serpiente\n",
        "        self.snake_coordinates = [ (1,1), (2,1) ] #comenzar en la esquina inferior izquierda\n",
        "        #Inicializar la grilla\n",
        "        self.grid = np.zeros( (self.grid_size, self.grid_size) ,dtype=np.uint8) + self.EMPTY\n",
        "        self.grid[0,:] = self.WALL; self.grid[:,0] = self.WALL; #pared en los bordes\n",
        "        self.grid[int(grid_size/2),3:(grid_size-3)] = self.WALL; #pared interior \n",
        "        self.grid[self.grid_size-1,:] = self.WALL; self.grid[:,self.grid_size-1] = self.WALL\n",
        "        for coord in self.snake_coordinates:\n",
        "            self.grid[ coord ] = self.SNAKE  #poner serpiente en la grilla\n",
        "        self.grid[2,2] = self.FOOD  #comenzar en la esquina superior derecha\n",
        "        #Distancia inicial a la comida\n",
        "        self.head_dist_to_food = self.grid_distance(self.snake_coordinates[-1],np.argwhere(self.grid==self.FOOD)[0] )\n",
        "        #Almacenar valores iniciales\n",
        "        self.init_grid = self.grid.copy()\n",
        "        self.init_snake_coordinates = self.snake_coordinates.copy()\n",
        "        \n",
        "        # El espacio de acción\n",
        "        self.action_space = spaces.Discrete(self.n_actions)\n",
        "        # El espacio de observación, \"posición\" son las coordenadas de la cabeza; \"dirección\" es hacia dónde se\n",
        "        #dirige la serpiente,\"cuadrícula\" contiene la información completa de la cuadrícula\n",
        "        self.observation_space = gym.spaces.Dict(\n",
        "            spaces={\n",
        "                \"position\": gym.spaces.Box(low=0, high=(self.grid_size-1), shape=(2,), dtype=np.int32),\n",
        "                \"direction\": gym.spaces.Box(low=-1, high=1, shape=(2,), dtype=np.int32),\n",
        "                \"grid\": gym.spaces.Box(low = 0, high = 3, shape = (self.grid_size, self.grid_size), dtype=np.uint8),\n",
        "            })\n",
        "    \n",
        "    def grid_distance(self,pos1,pos2):\n",
        "        return np.linalg.norm(np.array(pos1,dtype=np.float32)-np.array(pos2,dtype=np.float32))\n",
        "\n",
        "    def reset(self):\n",
        "        # Restablecer posiciones iniciales\n",
        "        self.stepnum = 0; self.last_food_step=0\n",
        "        self.grid = self.init_grid.copy()\n",
        "        self.snake_coordinates = self.init_snake_coordinates.copy()\n",
        "        #Distancia inicial a la comida\n",
        "        self.head_dist_to_food = self.grid_distance(self.snake_coordinates[-1],np.argwhere(self.grid==self.FOOD)[0] )\n",
        "        return self._get_obs()    \n",
        "             \n",
        "    def _get_obs(self):\n",
        "            direction = np.array(self.snake_coordinates[-1]) - np.array(self.snake_coordinates[-2])\n",
        "            #devolver la observación en el formato de self.observation_space\n",
        "            return {\"position\": np.array(self.snake_coordinates[-1],dtype=np.int32),\n",
        "                    \"direction\" : direction.astype(np.int32),\n",
        "                    \"grid\": self.grid}                  \n",
        "            \n",
        "    def step(self, action):\n",
        "        #Obtener dirección para la serpiente\n",
        "        direction = np.array(self.snake_coordinates[-1]) - np.array(self.snake_coordinates[-2])\n",
        "        if action == self.STRAIGHT:\n",
        "            step = direction #da un paso en la dirección a la que mira la serpiente\n",
        "        elif action == self.RIGHT:\n",
        "            step = np.array( [direction[1], -direction[0]] )  #Girar a la derecha\n",
        "        elif action == self.LEFT:\n",
        "            step = np.array( [-direction[1], direction[0]] )   #Girar a la izquierda\n",
        "        else:\n",
        "            raise ValueError(\"Se recibió una acción inválida={} que no es parte del espacio de acciones\".format(action))\n",
        "        #New head coordinate\n",
        "        new_coord = (np.array(self.snake_coordinates[-1]) + step).astype(np.int32)\n",
        "        #grow snake     \n",
        "        self.snake_coordinates.append( (new_coord[0],new_coord[1]) ) #convert to tuple so we can use it to index\n",
        "\n",
        "        \n",
        "        #Check what is at the new position\n",
        "        new_pos = self.snake_coordinates[-1]\n",
        "        new_pos_type = self.grid[new_pos]\n",
        "        self.grid[new_pos] = self.SNAKE #this position is now occupied by the snake\n",
        "        done = False; reward = 0 #by default the game goes on and no reward   \n",
        "        if new_pos_type == self.FOOD:\n",
        "            reward += self.REWARD_PER_FOOD\n",
        "            self.last_food_step = self.stepnum\n",
        "            #Put down a new food item\n",
        "            empty_tiles = np.argwhere(self.grid==self.EMPTY)\n",
        "            if len(empty_tiles):\n",
        "                new_food_pos=empty_tiles[np.random.randint(0,len(empty_tiles))]\n",
        "                self.grid[new_food_pos[0],new_food_pos[1]] = self.FOOD\n",
        "            else:\n",
        "                done = True #no more tiles to put the food to\n",
        "        else:\n",
        "            #If no food was eaten we remove the end of the snake (i.e., moving not growing)\n",
        "            self.grid[ self.snake_coordinates[0] ] = self.EMPTY\n",
        "            self.snake_coordinates = self.snake_coordinates[1:]\n",
        "            if  (new_pos_type == self.WALL) or (new_pos_type == self.SNAKE):\n",
        "                done = True #stop if we hit the wall or the snake\n",
        "                reward += self.REWARD_WALL_HIT #penalty for hitting walls/tail\n",
        "#             else:\n",
        "#                 reward += self.REWARD_PER_STEP\n",
        "                \n",
        "        #Update distance to food and reward if closer\n",
        "        head_dist_to_food_prev = self.head_dist_to_food\n",
        "        self.head_dist_to_food = self.grid_distance( self.snake_coordinates[-1],np.argwhere(self.grid==self.FOOD)[0] )\n",
        "        if head_dist_to_food_prev > self.head_dist_to_food:\n",
        "            reward += self.REWARD_PER_STEP_TOWARDS_FOOD #reward for getting closer to food\n",
        "        elif head_dist_to_food_prev < self.head_dist_to_food:\n",
        "            reward -= self.REWARD_PER_STEP_TOWARDS_FOOD #penalty for getting further\n",
        "        \n",
        "        #Stop if we played too long without getting food\n",
        "        if ( (self.stepnum - self.last_food_step) > self.MAX_STEPS_AFTER_FOOD ): \n",
        "            done = True    \n",
        "        self.stepnum += 1\n",
        "\n",
        "        return  self._get_obs(), reward, done, {}\n",
        "\n",
        "    def render(self, mode='rgb_array'):\n",
        "        if mode == 'console':\n",
        "            print(self.grid)\n",
        "        elif mode == 'rgb_array':\n",
        "            return self.snake_plot()\n",
        "        else:\n",
        "            raise NotImplementedError()\n",
        "\n",
        "    def close(self):\n",
        "        pass\n",
        "    \n",
        "    def snake_plot(self, plot_inline=False):\n",
        "        wall_ind = (self.grid==self.WALL)\n",
        "        snake_ind = (self.grid==self.SNAKE)\n",
        "        food_ind = (self.grid==self.FOOD)\n",
        "        #Create color array for plot, default white color\n",
        "        Color_array=np.zeros((self.grid_size,self.grid_size,3),dtype=np.uint8)+255 #default white\n",
        "        Color_array[wall_ind,:]= np.array([0,0,0]) #black walls\n",
        "        Color_array[snake_ind,:]= np.array([0,0,255]) #bluish snake\n",
        "        Color_array[food_ind,:]= np.array([0,255,0]) #green food  \n",
        "        #plot\n",
        "        if plot_inline:\n",
        "            fig=plt.figure()\n",
        "            plt.axis('off')\n",
        "            plt.imshow(Color_array, interpolation='nearest')\n",
        "            plt.show()\n",
        "        return Color_array\n",
        "\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwNLYiBNRxE5"
      },
      "source": [
        "### Probamos el entorno\n",
        "\n",
        "Utilizamos el algoritmo PPO (Proximal Policy Optimization), este tiene como objetivo mejorar la política actual del agente \"Serpiente\" de manera que maximice la recompensa obtenida. Para lograr esto, el algoritmo PPO utiliza una función de pérdida que tiene en cuenta tanto la recompensa obtenida por la política actual como el cambio en la misma relación con la política anterior.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4UFHcQCFIRk",
        "outputId": "59ccf765-1b10-4399-cb7c-aec4b22738f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 3.88     |\n",
            "|    ep_rew_mean     | 33.1     |\n",
            "| time/              |          |\n",
            "|    fps             | 1019     |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 2        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 4.5         |\n",
            "|    ep_rew_mean          | 54          |\n",
            "| time/                   |             |\n",
            "|    fps                  | 763         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 5           |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020654608 |\n",
            "|    clip_fraction        | 0.356       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.08       |\n",
            "|    explained_variance   | 0.000635    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.22e+03    |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.0625     |\n",
            "|    value_loss           | 2.51e+03    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 4.45        |\n",
            "|    ep_rew_mean          | 59          |\n",
            "| time/                   |             |\n",
            "|    fps                  | 721         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 8           |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015797198 |\n",
            "|    clip_fraction        | 0.311       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.04       |\n",
            "|    explained_variance   | 0.0535      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1e+03       |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.0582     |\n",
            "|    value_loss           | 2.74e+03    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 6.11        |\n",
            "|    ep_rew_mean          | 85.1        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 705         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 11          |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015556363 |\n",
            "|    clip_fraction        | 0.268       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.979      |\n",
            "|    explained_variance   | 0.0966      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.95e+03    |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.0564     |\n",
            "|    value_loss           | 3.21e+03    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 6.12        |\n",
            "|    ep_rew_mean          | 88.8        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 689         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 14          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.024073988 |\n",
            "|    clip_fraction        | 0.197       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.903      |\n",
            "|    explained_variance   | 0.138       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.59e+03    |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.0436     |\n",
            "|    value_loss           | 3.28e+03    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 6.57        |\n",
            "|    ep_rew_mean          | 129         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 619         |\n",
            "|    iterations           | 6           |\n",
            "|    time_elapsed         | 19          |\n",
            "|    total_timesteps      | 12288       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021527505 |\n",
            "|    clip_fraction        | 0.133       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.863      |\n",
            "|    explained_variance   | 0.19        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.44e+03    |\n",
            "|    n_updates            | 50          |\n",
            "|    policy_gradient_loss | -0.0269     |\n",
            "|    value_loss           | 3.3e+03     |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 6.87        |\n",
            "|    ep_rew_mean          | 127         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 579         |\n",
            "|    iterations           | 7           |\n",
            "|    time_elapsed         | 24          |\n",
            "|    total_timesteps      | 14336       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010391336 |\n",
            "|    clip_fraction        | 0.0764      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.819      |\n",
            "|    explained_variance   | 0.207       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.18e+03    |\n",
            "|    n_updates            | 60          |\n",
            "|    policy_gradient_loss | -0.0156     |\n",
            "|    value_loss           | 3.49e+03    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 6.85        |\n",
            "|    ep_rew_mean          | 130         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 587         |\n",
            "|    iterations           | 8           |\n",
            "|    time_elapsed         | 27          |\n",
            "|    total_timesteps      | 16384       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009792428 |\n",
            "|    clip_fraction        | 0.0829      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.761      |\n",
            "|    explained_variance   | 0.226       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.74e+03    |\n",
            "|    n_updates            | 70          |\n",
            "|    policy_gradient_loss | -0.0149     |\n",
            "|    value_loss           | 3.37e+03    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 6.58         |\n",
            "|    ep_rew_mean          | 128          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 594          |\n",
            "|    iterations           | 9            |\n",
            "|    time_elapsed         | 31           |\n",
            "|    total_timesteps      | 18432        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0064549604 |\n",
            "|    clip_fraction        | 0.0428       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.732       |\n",
            "|    explained_variance   | 0.275        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.61e+03     |\n",
            "|    n_updates            | 80           |\n",
            "|    policy_gradient_loss | -0.0086      |\n",
            "|    value_loss           | 2.92e+03     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 7.25        |\n",
            "|    ep_rew_mean          | 143         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 598         |\n",
            "|    iterations           | 10          |\n",
            "|    time_elapsed         | 34          |\n",
            "|    total_timesteps      | 20480       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005509835 |\n",
            "|    clip_fraction        | 0.0422      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.686      |\n",
            "|    explained_variance   | 0.269       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.71e+03    |\n",
            "|    n_updates            | 90          |\n",
            "|    policy_gradient_loss | -0.0077     |\n",
            "|    value_loss           | 3.68e+03    |\n",
            "-----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "env = Serpiente()\n",
        "env = make_vec_env(lambda: env, n_envs=1)\n",
        "\n",
        "model = PPO('MultiInputPolicy', env, verbose=1).learn(20000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "M4on9hxki58m",
        "outputId": "326a3e4b-c253-448d-9427-f043d0c59c23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1\n",
            "position= [2 2] direction= [0 1]\n",
            "reward= 99 done= False\n",
            "Step 2\n",
            "position= [1 2] direction= [-1  0]\n",
            "reward= 1 done= False\n",
            "Step 3\n",
            "position= [1 1] direction= [ 0 -1]\n",
            "reward= -1 done= False\n",
            "Step 4\n",
            "position= [2 1] direction= [1 0]\n",
            "reward= -1 done= False\n",
            "Step 5\n",
            "position= [2 2] direction= [0 1]\n",
            "reward= 1 done= False\n",
            "Step 6\n",
            "position= [1 2] direction= [-1  0]\n",
            "reward= 1 done= False\n",
            "Step 7\n",
            "position= [1 1] direction= [ 0 -1]\n",
            "reward= -1 done= False\n",
            "Step 8\n",
            "position= [2 1] direction= [1 0]\n",
            "reward= -1 done= False\n",
            "Step 9\n",
            "position= [2 2] direction= [0 1]\n",
            "reward= 1 done= False\n",
            "Step 10\n",
            "position= [1 2] direction= [-1  0]\n",
            "reward= 1 done= False\n",
            "Step 11\n",
            "position= [1 1] direction= [ 0 -1]\n",
            "reward= -1 done= False\n",
            "Step 12\n",
            "position= [2 1] direction= [1 0]\n",
            "reward= -1 done= False\n",
            "Step 13\n",
            "position= [2 2] direction= [0 1]\n",
            "reward= 1 done= False\n",
            "Step 14\n",
            "position= [1 2] direction= [-1  0]\n",
            "reward= 1 done= False\n",
            "Step 15\n",
            "position= [1 1] direction= [ 0 -1]\n",
            "reward= -1 done= False\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x432 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVQAAAFUCAYAAAB7ksS1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAE+ElEQVR4nO3aQWrkUBAFQX2j+1+5Zm9sg3GKAk3EsnvzhJqkFn1m5gLg7z62BwC8haACRAQVICKoABFBBYgIKkDk/unLc47/VAF8MjPnq89dqAARQQWICCpARFABIoIKEBFUgIigAkQEFSAiqAARQQWICCpARFABIoIKEBFUgIigAkQEFSAiqAARQQWICCpARFABIoIKEBFUgIigAkQEFSAiqAARQQWICCpARFABIoIKEBFUgIigAkQEFSAiqAARQQWICCpARFABIoIKEBFUgIigAkQEFSAiqAARQQWICCpARFABIoIKEBFUgIigAkQEFSAiqAARQQWICCpARFABIvf2gB2zPYDfmrO94DHz0t/jOe99Z99xoQJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASL39oANM9sLnnPO9oJnzPXil8ZruFABIoIKEBFUgIigAkQEFSAiqAARQQWICCpARFABIoIKEBFUgIigAkQEFSAiqAARQQWICCpARFABIoIKEBFUgIigAkQEFSAiqAARQQWICCpARFABIoIKEBFUgIigAkQEFSAiqAARQQWICCpARFABIoIKEBFUgIigAkQEFSAiqAARQQWICCpARFABIoIKEBFUgIigAkQEFSAiqAARQQWICCpARFABIoIKEBFUgIigAkQEFSBybw+gdrYHPOK887Gu67qumdmeQMSFChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiNzbA2jNzPYE+G+5UAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVIHJvD9hwztmeALyQCxUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChA5M7O9AeAVXKgAEUEFiAgqQERQASKCChARVIDIPyruGquoZA4BAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "##Manual testing\n",
        "import matplotlib.animation as animation\n",
        "from time import sleep\n",
        "\n",
        "env = Snake_game()\n",
        "env.reset()\n",
        "\n",
        " #Image for initial state\n",
        "fig, ax = plt.subplots(figsize=(6,6))\n",
        "plt.imshow(env.render(mode='rgb_array'))\n",
        "plt.axis('off')\n",
        "plt.savefig(\"snake_init.png\",dpi=150)\n",
        "\n",
        " #Framework to save animgif\n",
        "frames = []\n",
        "fps=24\n",
        "\n",
        "n_steps =15\n",
        "for step in range(n_steps):\n",
        "  print(\"Step {}\".format(step + 1))\n",
        "\n",
        "  obs, reward, done, info = env.step(0)\n",
        "  print('position=', obs['position'], 'direction=', obs['direction'])\n",
        "  print('reward=', reward, 'done=', done)\n",
        "  frames.append([ax.imshow(env.render(mode='rgb_array'), animated=True)])\n",
        "  if done:\n",
        "        print(\"Game over!\", \"reward=\", reward)\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mcoAniFELgT"
      },
      "source": [
        "\n",
        "2. Entrena agentes en entornos más complejos con stable-baselines/rl-baselines-zoo. Tener en cuenta:\n",
        "\n",
        "    * Google Colab tiene una limitante en cuanto a cantidad de recursos de CPU/GPU (incluido un \"rendimiento decreciente silencioso\"), lo cuál reduce la capacidad de entrenar distintos entornos.\n",
        "    * Si el entorno no está implementado en stable-baselines, debe hacerse un wrapper a mano, lo que puede ser sencillo o puede llevar algo más de trabajo, teniendo que tocar código subyacente de la librería. \n",
        "\n",
        "\\* pueden ser usando los agentes de stable-baselines, de rl-baselines-zoo, o bien utilizando algún otro algoritmo (incluso tabular)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Agente LunarLander**\n",
        "\n",
        "Entrenamos el agente LunarLander con el objetivo de controlar una nave espacial para aterrizarla de manera segura en la superficie lunar.\n",
        "El agente toma decisiones sobre qué acciones ejecutar en cada momento. Esto implica evaluar el estado actual de la simulación y seleccionar la acción que maximice la recompensa esperada en el futuro. A medida que el agente experimenta y juega el juego repetidamente, debería ser capaz de aprender a aterrizar la nave de manera más eficiente y segura.\n",
        "\n"
      ],
      "metadata": {
        "id": "eu1a1-hPsdNU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configuramos entorno y algoritmo DQN."
      ],
      "metadata": {
        "id": "15BsLeLR5RZ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update > /dev/null 2>&1\n",
        "!sudo apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!pip install rarfile --quiet\n",
        "!pip install stable-baselines3[extra] ale-py==0.7.4 --quiet\n",
        "!pip install box2d-py --quiet\n",
        "!pip install gym pyvirtualdisplay --quiet\n",
        "!pip install pyglet --quiet\n"
      ],
      "metadata": {
        "id": "GQ22sOeSJ0CB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importamos librerías necesarias\n",
        "import io\n",
        "import os\n",
        "import glob\n",
        "import torch\n",
        "import base64\n",
        "import stable_baselines3\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.common.results_plotter import ts2xy, load_results\n",
        "from stable_baselines3.common.callbacks import EvalCallback\n",
        "from stable_baselines3.common.env_util import make_atari_env\n",
        "\n",
        "import gym\n",
        "from gym import spaces\n",
        "from gym.wrappers import Monitor"
      ],
      "metadata": {
        "id": "kHVyJSTdQiA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAL0zEcYehpf"
      },
      "outputs": [],
      "source": [
        "nn_layers = [64,64] #configuración de la red neuronal.\n",
        "\n",
        "learning_rate = 0.001 ##Tamaño de paso con el que se realiza el descenso por gradiente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGmjc-CMW0nu"
      },
      "outputs": [],
      "source": [
        "# Crea el entorno del agente\n",
        "env = gym.make('LunarLander-v2')\n",
        "\n",
        "\n",
        "# Crea una carpeta temporal donde se almacenarán los archivos de registro del entrenamiento del agente\n",
        "log_dir = \"/tmp/gym/\"\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "env = stable_baselines3.common.monitor.Monitor(env, log_dir )\n",
        "\n",
        "# Inicia la grabación del video\n",
        "#env.start_video_recorder('video.mp4', (800, 600))\n",
        "\n",
        "callback = EvalCallback(env,log_path = log_dir, deterministic=True) #Para evaluar periódicamente el desempeño del agente y registrar los resultados.\n",
        "policy_kwargs = dict(activation_fn=torch.nn.ReLU,\n",
        "                     net_arch=nn_layers)\n",
        "model = DQN(\"MlpPolicy\", env,policy_kwargs = policy_kwargs,\n",
        "            learning_rate=learning_rate,\n",
        "            batch_size=1,  #por simplicidad no hacemos actualizaciones por lotes.\n",
        "            buffer_size=1, #tamaño de la experiencia del búfer de reproducción. \n",
        "            #Establecido en 1 ya que la actualización por lotes no se realiza\n",
        "            learning_starts=1, #comienzo de aprendizaje \n",
        "            gamma=0.99, #factor de descuento (entre 0 y 1)\n",
        "            tau = 1,  #coeficiente de actualización\n",
        "            target_update_interval=1, #actualizar la red de destino inmediatamente\n",
        "            train_freq=(1,\"step\"), #train the network at every step.\n",
        "            max_grad_norm = 10, #the maximum value for the gradient clipping\n",
        "            exploration_initial_eps = 1, #valor inicial de random action probability\n",
        "            exploration_fraction = 0.5, #fracción del período de entrenamiento durante el cual se reduce la tasa de exploración\n",
        "            gradient_steps = 1, #número de pasos del gradiente\n",
        "            seed = 1, #semilla para los generadores pseudoaleatorios\n",
        "            verbose=0) #Establecer verbose en 1 para observar los registros de entrenamiento\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.learn(total_timesteps=100000, log_interval=10, callback=callback)"
      ],
      "metadata": {
        "id": "5q-6fiFGB3Ie",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56e1dd9c-4d22-491f-b367-c9d4faefdf72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-510.91 +/- 143.09\n",
            "Episode length: 103.40 +/- 26.82\n",
            "New best mean reward!\n",
            "Eval num_timesteps=20000, episode_reward=-87.42 +/- 21.23\n",
            "Episode length: 82.00 +/- 16.94\n",
            "New best mean reward!\n",
            "Eval num_timesteps=30000, episode_reward=-11.44 +/- 224.79\n",
            "Episode length: 662.40 +/- 299.99\n",
            "New best mean reward!\n",
            "Eval num_timesteps=40000, episode_reward=-175.65 +/- 13.54\n",
            "Episode length: 383.00 +/- 169.33\n",
            "Eval num_timesteps=50000, episode_reward=-74.26 +/- 29.01\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "Eval num_timesteps=60000, episode_reward=-109.14 +/- 24.77\n",
            "Episode length: 472.20 +/- 191.91\n",
            "Eval num_timesteps=70000, episode_reward=-11.58 +/- 22.06\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "Eval num_timesteps=80000, episode_reward=21.27 +/- 108.71\n",
            "Episode length: 104.40 +/- 61.39\n",
            "New best mean reward!\n",
            "Eval num_timesteps=90000, episode_reward=-13.80 +/- 21.36\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "Eval num_timesteps=100000, episode_reward=150.25 +/- 86.42\n",
            "Episode length: 772.00 +/- 183.47\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<stable_baselines3.dqn.dqn.DQN at 0x7f7807f646d0>"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ploteamos el reward para cada episodio"
      ],
      "metadata": {
        "id": "eNKrt4aFhI8U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = ts2xy(load_results(log_dir), 'timesteps')  # Organizar los resultados registrados en un formato limpio para trazar.\n",
        "plt.plot(x,y)\n",
        "plt.ylim([-600, 300])\n",
        "plt.xlabel('Episodios')\n",
        "plt.ylabel('Recompensas por episodios');"
      ],
      "metadata": {
        "id": "ejmn84ZOF489",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "507b507c-5c8f-4b35-dae2-762fc0a2d986"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualización del rendimiento (REVISAR VIDEO)"
      ],
      "metadata": {
        "id": "bo56XmXHq25Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Plotting/Video functions\n",
        "from IPython.display import HTML\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as ipythondisplay\n",
        "from gym.wrappers.monitoring import video_recorder\n",
        "from IPython.display import HTML\n",
        "from IPython import display \n",
        "import glob\n",
        "import base64\n",
        "\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment\n",
        "and displaying it.\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay\n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else:\n",
        "    print(\"Could not find video\")\n",
        "\n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ],
      "metadata": {
        "id": "D7X7hF--risP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "SD-FSSuKoIJF",
        "uUUkScRxoIJQ"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "diplodatos_rl",
      "language": "python",
      "name": "diplodatos_rl"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}